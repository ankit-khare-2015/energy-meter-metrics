
# ğŸ“˜ Learning Project: Energy Analytics Dashboard with dbt + PostgreSQL + Grafana

This project is designed as a **learning journey** to help you understand how to build an end-to-end analytics pipeline using **dbt**, **PostgreSQL**, and **Grafana**, using simulated smart meter energy data.

> ![Dashboard](docs/report.png)

---

## ğŸ¯ Learning Goals

- Understand how dbt works (models, seeds, tests, sources)
- Practice building transformations from raw data to insights
- Explore PostgreSQL as a warehouse for analytics
- Build Grafana dashboards from SQL models
- Learn about data quality checks and incremental models

---

## ğŸ“¦ Project Structure Overview

```
.
â”œâ”€â”€ docker-compose.yml        # Runs Postgres, dbt, Grafana locally
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ dbt_project.yml       # dbt config file
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/          # Clean raw data into stg models
â”‚   â”‚   â”œâ”€â”€ fact/             # Business logic (e.g., energy summaries)
â”‚   â”‚   â””â”€â”€ schema.yml        # Define sources, tests, and descriptions
â”‚   â”œâ”€â”€ data/                 # Seed data in CSV form
â”‚   â””â”€â”€ config/profiles.yml   # dbt connection settings
â””â”€â”€ grafana/ (optional)       # Prebuilt dashboards (optional)
```

---

## ğŸš€ Step-by-Step Instructions

### ğŸ³ 1. Start Your Local Stack

```bash
docker-compose up -d
```

This brings up:
- PostgreSQL (data warehouse)
- dbt (transformation engine)
- Grafana (dashboard UI)

---

### ğŸ” 2. Learn dbt: Core Commands

#### ğŸ§ª Load Raw Data into Postgres (data from /dbt_project/data/* is loaded in postgres database)

## ğŸŒ± Seed Data Overview

The project uses simulated smart meter data loaded via `dbt seed`. These CSV files are located in `dbt_project/data/` and represent the raw input tables in the PostgreSQL database.

| Seed File              | Description                                      |
|------------------------|--------------------------------------------------|
| `households.csv`       | Household metadata including city, meter ID, and installation date |
| `smart_meter_readings.csv` | Hourly readings of energy consumption, voltage, and current from each meter |
| `tariff_rates.csv`     | Tariff pricing over date ranges (for cost calculation) |

These tables are referenced as **sources** in dbt models and form the basis for all downstream transformations.

Run this to load them:

```bash
docker exec -it dbt bash
cd /usr/app
dbt seed
```

#### ğŸ§± Transform Your Data
```bash
dbt run
```

#### ğŸ›¡ï¸ Test Data Quality
```bash
dbt test
```

#### ğŸ“š View Lineage & Docs
```bash
dbt docs generate
dbt docs serve
```

> ![Generated Docs](docs/dbt-docs.png)

Explore your DAG, model descriptions, and test results.

---

## ğŸ“Š What You Will Build

### Dashboards in Grafana (http://localhost:3000)
Login: `admin` / `admin`

| Report                        | Description                                      | Model/Table              |
|------------------------------|--------------------------------------------------|---------------------------|
| Daily Energy & Cost Trends   | kWh & cost over time                             | `fct_daily_energy`, `fct_energy_cost` |
| Top 5 Households by Usage    | Highest consumers by kWh                         | `fct_daily_energy`        |
| City-wise Avg Consumption    | Avg kWh per city                                 | `fct_daily_energy`        |
| Voltage Anomaly Alerts       | Detect when avg voltage is out of range          | `fct_daily_energy`        |
| Cost Efficiency Overview     | Cost per kWh                                     | `fct_energy_cost`         |

---

## ğŸ§  Explore These Concepts

### ğŸ“Œ dbt Concepts

- `seeds`: load CSVs into your warehouse
- `models`: SQL files that transform data
- `sources`: define upstream tables (like `raw.smart_meter_readings`)
- `tests`: catch bad data early (e.g., `not_null`, `unique`)
- `ref()` and `source()`: create dependency graph
- `incremental models`: only load new data (advanced)

---

## ğŸ”„ Suggested Next Steps

- Add **incremental logic** to `fct_daily_energy`
- Simulate streaming using **Kafka â†’ PostgreSQL**
- Add more **anomaly detection logic**
- Create alerts in **Grafana**
- Deploy your pipeline to **Cloud (Azure/Snowflake/BigQuery)**
- Explore tools like **Metabase** or **Apache Superset**
- Automate tests using **CI/CD (GitHub Actions)**

---

## ğŸ‘¨â€ğŸ’» Created By

Built as a self-learning project by [Ankit Khare](https://www.linkedin.com/in/ankit-khare-2015)  
_Data Architect â€¢ Azure â€¢ AI â€¢ Streaming â€¢ dbt â€¢ Kafka â€¢ Postgres_

